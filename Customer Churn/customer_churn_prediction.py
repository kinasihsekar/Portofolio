# -*- coding: utf-8 -*-
"""Customer Churn Prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1PQ8nX2sQ167WUcyRruOEcm_40u8XzZP6

**PROJECT**


> project ini merupakan solusi data science untuk permasalahan *default risk* atau risiko gagal bayar menggunakan dataset Home Credit Defaults Risk dengan langkah-langkah mengikuti framework CRISP-DM.

# Business Understanding

>Risiko kredit adalah hal tak terbantahkan dari sebuah perusahan penyedia layanan pembiayaan. Salah satu risiko kredit yang sering dialami yaitu gagalnya nasabah untuk membayar kredit (*default risk*). Business problem yang dihadapi adalah tingginya persentase gagal bayar kredit sebesar 8% atau setara dengan 20 ribu orang, yang dapat berdampak negatif pada keuangan perusahaan dan reputasi perusahaan di mata nasabah. Salah satu perusahaan yakni Home Credit berupaya untuk meningkatkan performa hasil analisisnya dalam risiko kredit. Home Credit menggunakan berbagai data alternatif--termasuk telekomunikasi dan informasi transaksional-- untuk memprediksi kemampuan pembayaran klien mereka.

>Proyek ini bertujuan untuk memprediksi aspek yang berpengaruh terhadap ketidakmampuan nasabah dalam membayar kredit menggunakan model machine learning. Dengan diketahui faktor tersebut, maka Home Credit dapat memprediksi apakah client akan gagal bayar kredit atau tidak.

# Data Understanding
"""

#import libraries untuk olah data
import pandas as pd
import numpy as np
#import libraries untuk visualisasi
import seaborn as sns
import matplotlib.pyplot as plt

#menampilkan maksimum 200 kolom pada output data frame
pd.set_option('display.max_columns', None)

#menghilangkan pesan peringatan (warning) yang muncul saat menjalankan program
import warnings
warnings.filterwarnings("ignore")

from google.colab import drive
drive.mount('/content/drive')

data=pd.read_csv('/content/drive/MyDrive/application_train.csv')
data.head()

data.shape

data.columns

""">**Summary**
* Dataset di atas berisikan data statis untuk semua peminjam. Satu baris mewakili satu pinjaman dalam sampel data.
* Dateset terdiri dari 307511 baris dan 122 kolom. Dengan mengetahui ukuran dataset, akan didapatkan gambaran tentang seberapa besar data yang harus diproses dan mempersiapkan langkah selanjutnya dalam melakukan analisis atau pembuatan model machine learning.
* Variabel target ada pada kolom bernama TARGET yang berisi mengenai kemampuan seorang peminjam yang ditulis dalam angka 1 (untuk nasabah yang kesulitan/keterlambatan dalam membayar kembali pinjaman) dan 0 (bagi nasabah yang dapat membayar kembali pinjaman/tepat waktu dalam membayar pinjaman).
* Terdapat kolom DAYS_BIRTH, DAYS_ID_PUBLISH, DAYS_REGISTRATION, DAYS_EMPLOYED, dan DAYS_LAST_PHONE_CHANGE yang berisikan hitungan hari dengan nilai negatif sehingga perlu diganti format datanya.

#Data Preparation

## Formatting Data

Data akan dibenahi formatnya dan dirubah menjadi tahun untuk memudahkan analisis selanjutnya.
"""

#membenahi format data
data['DAYS_BIRTH'] = (data['DAYS_BIRTH']*-1/365)
data["DAYS_ID_PUBLISH"] = (data['DAYS_ID_PUBLISH']*-1/365)
data["DAYS_REGISTRATION"] = (data['DAYS_REGISTRATION']*-1/365)
data["DAYS_LAST_PHONE_CHANGE"] =(data['DAYS_LAST_PHONE_CHANGE']*-1/365)
data['DAYS_EMPLOYED'] =abs(data['DAYS_EMPLOYED']/365)

"""Selanjutnya akan dilakukan handling missing value.

## Handling Missing Value

Banyaknya atau kuantitas misssing value dalam sebuah dataset penting untuk diperhatikan. Hal tersebut disebabkan cara menangani missing value pada kolom dengan hanya 1% missing value perlu dibedakan dengan kolom berisi 90% missing value.
"""

data.isnull().sum()

"""Meskipun dapat dilihat dengan jelas terdapat beberapa kolom dengan missing value, hasil tersebut tidak terlalu membantu. Karena dataset yang digunakan sangat besar yang tentunya  memiliki missing data, perlu kita telusuri lebih jauh. Akan digunakan list untuk menampilkan kolom dengan missing value seperti di bawah ini."""

data.columns[data.isnull().any()].tolist()

# membuat fungsi untuk menunjukkan info missing value
def missing_values_info(data):

    # menemukan missing values pada tiap kolom
    count_missing = data.isnull().sum()

    # membuat missing values dalam bentuk percent
    percent_missing = (100 * count_missing / data.shape[0]).round(1)

    # membuat dataframe dengan hasil di atas
    missing_df = pd.DataFrame({'Count Missing':count_missing,
                               'Percent Missing':percent_missing})

    # mengurutkan nilai berdasarkan persentase jumlah data yang missing dari yang terbesar
    missing_df = missing_df.sort_values('Percent Missing', ascending=False)

    # menghitung total persentase dari banyaknya kolom dengan missing value dan membandingkannya dengan total kolom yang ada
    missing_values_list = data.columns[data.isnull().any()].tolist()
    cols_missing_vals = len(missing_values_list)
    df_cols = data.shape[1]

    print(f'Kolom dengan Missing Values: {cols_missing_vals}')
    print(f'Total Kolom: {df_cols}')
    print(f'Persentase dari Banyaknya Kolom dengan Missing Values: {round(cols_missing_vals/df_cols*100,1)}%')

    return missing_df

missing_values_info(data).head(20)

"""Terlihat di atas bahwa pada dataset separuh dari itu berisi kolom dengan missing value, yakni sebanyak 67 dari 122 kolom terdapat missing value. Selanjutnya, kolom yang berisi missing value lebih dari 50% akan didrop karena apabila diisi dengan suatu nilai dapat menyebabkan bias pada hasil prediksi."""

#menghapus kolom berdasarkan persentase missing value
def drop_missing_values(dataframe, threshold):

    # membuat list dari kolom dengan jumlah missing value melebihi thershold
    to_drop = [col for col in dataframe if
               (dataframe[col].isnull().sum()/len(dataframe) >= threshold)]

    print('Kolom yang didrop: ' , (len(to_drop)))
    # drop kolom
    dataframe = dataframe.drop(columns=to_drop)
    print('Shape: ', dataframe.shape)
    return dataframe

data = drop_missing_values(data, .5)

# menampilkan jumlah missing value
missing_values_info(data).head(20)

"""Kolom yang didrop sebanyak 41 kolom dan tersisa 81 kolom dengan 26 diantaranya masih terdapat missing value. Persentase dari jumlah kolom dengan missing value menurun yakni menjadi 32%. Kolom-kolom tersebut akan menggunakan fungsi fillna untuk mengisi missing valuenya. Sebelum itu, akan dibagi datanya menjadi data kategorik dan numerik."""

#untuk data pada kolom CODE_GENDER ganti XNA menjadi missing untuk diimpute
data['CODE_GENDER'].replace('XNA', np.nan, inplace=True)

# Mengganti XNA menjadi NaN pada kolom ORGANIZATION_TYPE
data['ORGANIZATION_TYPE'].replace('XNA', np.nan, inplace=True)

categorical_vars = [var for var in data.columns if data[var].isnull().any() and
                   data[var].dtypes=='O']
categorical_vars

numeric_vars=data.columns[data.isnull().any()].tolist()
numeric_vars = [e for e in numeric_vars if e not in categorical_vars]
numeric_vars

"""Pada data numerik, kolom dengan missing value akan diisi dengan nilai mediannya karena data tidak berdistribusi normal."""

from sklearn.impute import SimpleImputer
imputer = SimpleImputer(missing_values=np.nan, strategy='median')
data[numeric_vars] = imputer.fit_transform(data[numeric_vars])

missing_values_info(data).head(20)

data[categorical_vars].describe()

from sklearn.impute import SimpleImputer
imputer = SimpleImputer(strategy='most_frequent')
imputer.fit(data[categorical_vars])
data[categorical_vars] = imputer.transform(data[categorical_vars])

data = pd.DataFrame(data)

missing_values_info(data)

data.shape

"""Ukuran data setelah handling missing value adalah 307511 baris dan 81 kolom.

## Checking Outliers and Anomali Handling

Akan dilihat nilai outliers pada kolom numerik menggunakan boxplot. Kolom numerik setelah handling missing values terdapat 68 kolom.
"""

nums=[i for i in data.columns if data[i].dtypes != 'object']

len(nums)

plt.figure(figsize=(10,50))
for i in range(0, len(nums)):
    plt.subplot(12, 6, i+1)
    sns.boxplot(y=data[nums[i]], color='green', orient='v')
    #memperbaiki tata letak tabel agar lebih rapi dan sesuai dengan ukuran halaman yang ditentukan dapat menjadikan gambar tidak terpotong saat diunduh
    plt.tight_layout()

"""Outliers di atas masih masuk akal, seperti outliers pada jumlah kredit dan jumlah anuitas yang diperkirakan memiliki hubungan linier sehingga outliers satu sama lain berhubungan. Selain itu pada kolom jumlah anak yang dimiliki. Mempunyai anak mencapai 20 memungkinkan, meskipun jarang ditemui tetapi tetap mungkin terjadi."""

sns.boxplot(y=data['DAYS_EMPLOYED'], data=data,orient = 'v')

"""Dalam hal ini terdapat nilai anomali yakni 1000 tahun pada kolom DAYS_EMPLOYED, hal tersebut telah dijelaskan pada data analysis, dan pada tahap ini, baris tersebut akan dihapus."""

data['DAYS_EMPLOYED'].max()

data['DAYS_EMPLOYED'].value_counts().max()

"""Banyaknya baris pada DAYS_EMPLOYED yang memiliki nilai max ada 55.374 baris. Baris tersebut akan dihapus."""

max_value = data['DAYS_EMPLOYED'].max()
filter = data['DAYS_EMPLOYED']==max_value
data.drop(data[filter].index, inplace=True)

data.shape

"""Ukuran data menjadi 252.137 baris dan 81 kolom."""

sns.boxplot(y=data['DAYS_EMPLOYED'], data=data,orient = 'v')

"""Terlihat dari boxplot sudah tidak ada nilai anomali dan nilai maksimalnya menjadi 49 tahun."""

data['DAYS_EMPLOYED'].max()

"""# Data Analysis

Selanjutnya, setelah dilakukan data understanding, akan dilakukan data analysis untuk menganalisis dataset yang ada. Langkah pertama dalam data analysis yakni *univariate analysis*. Penjelasan lebih lanjut adalah sebagai berikut.

## Univariate Analysis

*Univariate analysis* berisi analisis data satu variabel.

### TARGET
"""

data['TARGET'].value_counts()

#Percentage calculation
print("Percentage : ")
(data["TARGET"].value_counts()/data["TARGET"].count())*100

plt.figure(figsize=(8,6))
data['TARGET'].value_counts(normalize=True).plot(kind='pie', autopct="%.1f%%")
plt.title('Proportions of TARGET', fontsize=15)

"""- Jumlah nasabah yang tidak akan melunasi pinjaman tepat waktu: 230302 orang , (8.7 %)
- Jumlah nasabah yang akan mengembalikan pinjaman tepat waktu: 21835 orang, (91.3%)
- Terdapat ketidakseimbangan data.

**Function to plot the Stacked Bar Plot**

Fungsi ini digunakan untuk membuat jenis grafik batang yang mewakili kontribusi proporsional dari masing-masing titik data dibandingkan dengan total.
"""

def stack_plot(data, xtick, col2='TARGET', col3='total') :
  ind = np.arange(data.shape[0])

  if len (data[xtick].unique())<5:
    plt.figure(figsize=(5,5))

  elif len (data[xtick].unique())>5 & len(data[xtick].unique())<10:
    plt.figure(figsize=(7,7))
  else :
    plt.figure(figsize=(15,15))
  p1= plt.bar(ind, data[col3].values)
  p2= plt.bar(ind, data[col2].values)

  plt.ylabel('Pinjaman')
  plt.title('Jumlah keterlambatan pembayaran pinjaman VS pembayaran tepat waktu')
  plt.xticks(ticks=ind, rotation=90, labels = list(data[xtick].values))
  plt.legend((p1[0], p2[0]), ('pembayaran tepat waktu', 'keterlambatan pembayaran'))
  plt.show()

"""**Function to plot Univariated Bar Plot**

Fungsi ini digunakan untuk visualisasi data univariat pada sumbu dua dimensi. Sumbu x adalah sumbu kategori yang menunjukkan kategori, sedangkan sumbu y adalah sumbu nilai yang menunjukkan nilai numerik dari kategori tersebut, yang ditunjukkan oleh panjang grafik batang.
"""

def univariate_barplots(data, col1, col2='TARGET', top=False):

    temp = pd.DataFrame(data.groupby(col1)[col2].agg(lambda x: x.eq(1).sum())).reset_index()


    temp['total'] = pd.DataFrame(data.groupby(col1)[col2].agg(total='count')).reset_index()['total']
    temp['Avg'] = pd.DataFrame(data.groupby(col1)[col2].agg(Avg='mean')).reset_index()['Avg']

    temp.sort_values(by=['total'],inplace=True, ascending=False)

    if top:
        temp = temp[0:top]

    stack_plot(temp, xtick=col1, col2=col2, col3='total')
    print(temp.head(5))
    print("="*50)
    print(temp.tail(5))

"""### Name_Contract_Type"""

data['NAME_CONTRACT_TYPE'].value_counts()

univariate_barplots(data, 'NAME_CONTRACT_TYPE', 'TARGET', False)

"""Sebagian besar orang mengambil pinjaman dalam bentuk pinjaman tunai (cash loans) daripada pinjaman bergulir (revolving loans) seperti kartu kredit. Dari 226.224 peminjam cash loans, terdapat 20.371  peminjam yang mengalami keterlambatan pembayaran.

### Code_Gender
"""

univariate_barplots(data, 'CODE_GENDER', 'TARGET', False)

"""- Wanita mengambil lebih banyak pinjaman dibandingkan dengan pria.
- Dan ada saat yang sama, wanita sedikit lebih mampu membayar kembali pinjaman dibandingkan dengan pria. Pria mampu membayar kembali pinjaman mereka dalam 90% kasus, sedangkan wanita hanya mampu membayar dalam 93% kasus.

### Flag_Own_Car
"""

univariate_barplots(data, 'FLAG_OWN_CAR', 'TARGET', False)

"""Sebagian besar pemohon pinjaman tidak memiliki mobil.
Namun, tidak banyak perbedaan status pembayaran pinjaman untuk pelanggan berdasarkan informasi ini (masing-masing 9% dan 7%) sehingga dapat menyimpulkan bahwa fitur ini tidak terlalu berguna.

###Flag_Own_Realty
"""

univariate_barplots(data, 'FLAG_OWN_REALTY', 'TARGET', False)

"""Sebagian besar pemohon pinjaman memiliki flat/rumah.
Namun, tidak banyak perbedaan status pembayaran pinjaman untuk nasabah berdasarkan informasi ini (masing-masing 8,6% dan 8,7%) sehingga dapat menyimpulkan bahwa fitur ini tidak terlalu berguna.

### Cnt_Children
"""

univariate_barplots(data, 'CNT_CHILDREN', 'TARGET', False)

"""Pelamar yang tidak memiliki anak mengambil jumlah pinjaman yang jauh lebih tinggi.
Namun,tidak banyak perbedaan status pembayaran pinjaman untuk nasabah berdasarkan informasi ini sehingga dapat menyimpulkan bahwa fitur ini tidak terlalu berguna.

### Amt_Income_Total
"""

income_data = data.groupby('SK_ID_CURR').agg({'AMT_INCOME_TOTAL':'mean'}).reset_index()
income_data.head(2)

income_data_final = pd.merge(data, income_data, on='SK_ID_CURR', how='left')

approved_income = income_data_final[income_data_final['TARGET']==0]['AMT_INCOME_TOTAL_x'].values

rejected_income = income_data_final[income_data_final['TARGET']==1]['AMT_INCOME_TOTAL_x'].values

from prettytable import PrettyTable

x=PrettyTable()
x.field_names=["Percentile", "Pembayaran Tepat Waktu", "Keterlambatan Pembayaran"]
for i in range (0,101,5):
  x.add_row([i,np.round(np.percentile(approved_income,i), 3),\
               np.round(np.percentile(rejected_income,i), 3)])
print(x)

"""Kita dapat melihat bahwa hingga persentil ke-40, baik pinjaman yang dibayar tepat waktu maupun yang dibayar terlambat sebagian besar memiliki nilai yang sama.
Akan tetapi, apabila dilihat lebih jauh, saat penghasilan meningkat peluang pembayaran pinjaman tepat waktu juga meningkat.

### Amt_Credit
"""

for i in data.groupby('SK_ID_CURR',as_index=False).size():
  if i.isdigit():
    j=int(i)
    print(j)

data[data['SK_ID_CURR']==100002]

approved_loan_credit = data[data['TARGET']==0]['AMT_CREDIT'].values

rejected_loan_credit = data[data['TARGET']==1]['AMT_CREDIT'].values

plt.boxplot([approved_loan_credit, rejected_loan_credit])

plt.title('Jumlah Kredit untuk Setiap Pinjaman')
plt.xticks([1,2],('Pembayaran Tepat Waktu','Pembayaran Terlambat'))
plt.ylabel('Jumlah Credit')

plt.grid()
plt.show()

"""- Boxplot di atas menunjukkan bahwa nilai median dari jumlah kredit peminjam yang mampu mengembalikan pinjaman tepat waktu sedikit lebih besar dari nilai median peminjam yang terlambat.
- Hal tersebut artinya peminjam dengan jumlah kredit yang lebih tinggi memiliki kemungkinan yang sedikit lebih tinggi untuk mampu membayar pinjaman tepat waktu dibandingkan peminjam dengan jumlah kredit yang lebih rendah.

### Amt_Annuity
"""

capable_loan_annuity = data[data['TARGET']==0]['AMT_ANNUITY'].values
not_capable_loan_annuity = data[data['TARGET']==1]['AMT_ANNUITY'].values

plt.figure(figsize=(10,3))
sns.distplot(capable_loan_annuity,hist=False,label="Pembayaran Tepat Waktu", color='green')
sns.distplot(not_capable_loan_annuity,hist=False,label="Pembayaran Terlambat", color='red')
plt.legend()
plt.show()

"""Kebanyakan orang membayar anuitas di bawah 50.000 untuk pinjaman

### Name_Type_Suite
"""

data['NAME_TYPE_SUITE'].unique()

univariate_barplots(data, 'NAME_TYPE_SUITE','TARGET', False)

"""- Orang yang datang tanpa didampingi sesorang adalah kasus terbanyak, dengan 87% dari peminjam membayar tepat waktu.
- Dalam kemampuan pembayaran (pembayaran tepat waktu ataupun pembayaran terlambat), 'Unaccompanied' adalah kelas mayoritas dalam hal ini.
- Bar chart di sini turun sangat tajam, artinya ada banyak variabilitas.

###Name_Income_Type
"""

data['NAME_INCOME_TYPE'].unique()

data['NAME_INCOME_TYPE'].fillna('Data_Not_Available', inplace=True)
univariate_barplots(data,'NAME_INCOME_TYPE','TARGET',False)

"""- Orang-orang yang bekerja (Working) mengambil pinjaman paling banyak sedangkan pekerja komersial (Commercial
Associates), pensiunan (Pensioners), dan pegawai negeri (State Servants) mengambil jumlah pinjaman yang jauh lebih sedikit.
- Terlihat bahwa sedikit titik data yang terkait dengan pengangguran (Unemployed), pelajar (student), pengusaha (Businessman), dan
perempuan cuti hamil (Maternity leave). Ada banyak variabilitas dalam hal ini.
- Satu pengamatan menarik di sini adalah kenyataan bahwa pinjaman yang diberikan kepada siswa dan pengusaha yang telah mengajukan, mereka telah dianggap mampu membayar kembali pinjaman dengan tepat waktu.

### Name_Education_Type
"""

data['NAME_EDUCATION_TYPE'].unique()

data['NAME_EDUCATION_TYPE'].fillna('Data_Not_Available', inplace=True)
univariate_barplots(data,'NAME_EDUCATION_TYPE','TARGET',False)

"""- Sekali lagi, ada banyak variasi dalam  jenis pendidikan peminjam.
- Orang dengan Secondary/Secondary Special(Menengah Khusus/Menengah) sebagai tingkat pendidikan tertinggi dalam jumlah pinjaman dan mereka juga pembayaran terlambat paling tinggi.
- Secara persentase semakin tinggi tingkat pendidikan seseorang, kemampuan pembayaran pinjamannya juga meningkat.

### Name_Family_Status
"""

data['NAME_FAMILY_STATUS'].unique()

data['NAME_FAMILY_STATUS'].fillna('Data_Not_Available',inplace=True)
univariate_barplots(data, 'NAME_FAMILY_STATUS','TARGET',False)

"""- Ada variabilitas di antara Status Keluarga peminjam tetapi tidak banyak
variabilitas jika kelas mayoritas (Married) diabaikan.
- Orang yang sudah menikah (Married) mengajukan pinjaman paling banyak dan jumlah orang yang dianggap tidak mampu membayar pinjaman juga yang tertinggi.

### Name_Housing_Type
"""

data['NAME_HOUSING_TYPE'].unique()

data['NAME_HOUSING_TYPE'].fillna('Data_Not_Available',inplace=True)
univariate_barplots(data, 'NAME_HOUSING_TYPE','TARGET',False)

"""- Orang yang tinggal di Rumah/Apartemen (House / apartment) mengajukan pinjaman paling banyak dan paling banyak
orang yang dianggap tidak mampu membayar pinjaman
- Jika dilihat persentasenya, orang yang tinggal di apartemen sewaan (Rented apartment) memiliki peluang tertinggi
untuk kesulitan membayar pinjaman/melakukan keterlambatan pembayaran.

### Days_Birth
"""

capable_days_birth = data[data['TARGET']==0]['DAYS_BIRTH'].values
not_capable_days_birth = data[data['TARGET']==1]['DAYS_BIRTH'].values

plt.figure(figsize=(10,3))
plt.hist(data['DAYS_BIRTH'].values, bins=10, edgecolor='black', color='blue')
plt.title('Usia Peminjam (dalam tahun) pada Saat Mengajukan Pinjaman')
plt.xlabel('Rentang Usia')
plt.ylabel('Jumlah Peminjam')
plt.show()

plt.figure(figsize=(10,3))
plt.hist(capable_days_birth, bins=10, edgecolor = 'black', color = 'green')
plt.title('Usia Peminjam (dalam tahun) pada Saat Mengajukan Pinjaman yang Membayar Tepat Waktu')
plt.xlabel('Rentang Usia')
plt.ylabel('Jumlah Peminjam')
plt.show()

plt.figure(figsize=(10,3))
plt.hist(not_capable_days_birth, bins=10, edgecolor='black', color='red')
plt.title('Usia Peminjam (dalam tahun) pada Saat Mengajukan Pinjaman yang Membayar Terlambat ')
plt.xlabel('Rentang Usia')
plt.ylabel('Jumlah Peminjam')
plt.show()

"""- Sebagian besar orang yang mengajukan pinjaman berada dalam kisaran 35-40 tahun demikian halnya dengan rentang usia 40-45 tahun. Sedangkan, jumlah peminjam usia <25 tahun atau usia >65 tahun sangat sedikit.
- Orang-orang yang dianggap paling mampu mengembalikan pinjaman adalah orang-orang di rentang usia 35-40 tahun dan 40-45 tahun.
- Orang yang berusia dalam rentang 25-30 tahun dan 30-35 tahun memiliki peluang besar memiliki kesulitan dalam mengembalikan pinjaman.

### Days_Registration
"""

capable_days_registration = data[data['TARGET']==0]['DAYS_REGISTRATION'].values
not_capable_days_registration = data[data['TARGET']==1]['DAYS_REGISTRATION'].values

plt.figure(figsize=(10,3))
sns.distplot(capable_days_registration,hist=False,label="Pembayaran Tepat Waktu", color='green')
sns.distplot(not_capable_days_registration,hist=False,label="Pembayaran Terlambat", color='red')
plt.legend()
plt.show()

"""- Sebagian besar peminjam telah mengubah pendaftaran mereka kurang dari 41 tahun sebelum
permohonan pinjaman, sedangkan dalam banyak kasus kurang dari 13 tahun.

### Cnt_Fam_Members
"""

capable_family_members = data[data['TARGET']==0]['CNT_FAM_MEMBERS'].values
not_capable_family_members = data[data['TARGET']==1]['CNT_FAM_MEMBERS'].values

plt.figure(figsize=(10,3))
sns.distplot(capable_family_members, hist=False, label="Pembayaran Tepat Waktu", color='green')
sns.distplot(not_capable_family_members, hist=False, label="Pembayaran Terlambat", color='red')
plt.legend()
plt.show()

"""Sebagian besar peminjam memiliki 2 anggota keluarga dan sangat sedikit pelamar dengan >5
anggota keluarga

### Weekday_Appr_Process_Start
"""

univariate_barplots(data, 'WEEKDAY_APPR_PROCESS_START', 'TARGET', False)

"""Hal ini sangat menarik karena jumlah peminjam yang tersebar hampir merata
sepanjang hari kerja (Senin/Monday-Jumat/Friday) (sekitar 40 ribu peminjam per hari) (16-17%), sedangkan jumlah peminjam sangat rendah pada hari Minggu (Sunday)

###EXT_SOURCE_2
"""

capable_ext_source_2 = data[data['TARGET']==0]['EXT_SOURCE_2'].values
not_capable_ext_source_2 = data[data['TARGET']==1]['EXT_SOURCE_2'].values

plt.figure(figsize=(10,3))
sns.distplot(capable_ext_source_2,hist=False,label="Pembayaran Tepat Waktu", color='blue')
sns.distplot(not_capable_ext_source_2,hist=False,label="Pembayaran Terlambat", color='red')
plt.legend()
plt.show()

"""Terlihat bahwa terdapat perbedaan yang signifikan yang dapat dilihat dari plot PDF sehingga feature ini akan menjadi feature yang penting

###EXT_SOURCE_3
"""

capable_ext_source_3 = data[data['TARGET']==0]['EXT_SOURCE_3'].values
not_capable_ext_source_3 = data[data['TARGET']==1]['EXT_SOURCE_3'].values

plt.figure(figsize=(10,3))
sns.distplot(capable_ext_source_3,hist=False,label="Pembayaran Tepat Waktu", color='blue')
sns.distplot(not_capable_ext_source_3,hist=False,label="Pembayaran Terlambat", color='red')
plt.legend()
plt.show()

"""terlihat bahwa data terpisahkan dengan cukup baik sehingga feature ini akan menjadi feature penting

## Bivariate Analysis

Pada bagian ini akan dianalisis dua variabel dari dataset.

### NAME_CONTRACT_TYPE & AMT_CREDIT
"""

sns.catplot(x="NAME_CONTRACT_TYPE", y="AMT_CREDIT", hue="CODE_GENDER" ,col="TARGET",\
            data=data,color = "BLUE",kind="box", height=7, aspect=.7);

"""Ini menunjukkan bahwa Pria & Wanita dengan Pinjaman Tunai (Cash Loan) memiliki peluang lebih tinggi untuk dianggap mampu membayar kembali pinjaman berdasarkan Jumlah Kredit (Credit Amount) mereka

### NAME_INCOME_TYPE & AMT_CREDIT
"""

sns.catplot(x="AMT_CREDIT", y="NAME_INCOME_TYPE", col="TARGET",
            data=data,color = "BLUE",kind="box", height=7, aspect=.7);

"""Ini menunjukkan bahwa peminjam dengan Nilai Jumlah Kredit Lebih Tinggi di berbagai jenis pendapatan memiliki kemungkinan lebih tinggi dianggap mampu melakukan pelunasan pinjaman, terutama di
kasus 'Pengangguran', 'Mahasiswa' dan 'Pengusaha'

### AMT_CREDIT & AMT_ANNUITY
"""

sns.scatterplot(
    data=data, x="AMT_CREDIT", y="AMT_ANNUITY", hue="TARGET",\
    sizes=(20, 200)
)

"""- Hal ini menunjukkan bahwa Jumlah Kredit (AMT_CREDIT) dan Jumlah Anuitas (AMT_ANNUITY) berbanding lurus
satu sama lain. Jika Jumlah Kredit tinggi, Jumlah Anuitas yang sama juga akan tinggi.
- Terlihat grafik yang hampir linear.

#Modelling

Model machine learning dapat bekerja hanya pada data numeri. Oleh karena itu, akan dilakukan proses data encoding untuk memodelkan.

## Data Encoding
"""

#menampilkan informasi dari data
data.info()

"""Berdasarkan output, dapat diketahui bahwa jumlah variabel yang bertipe kategorik sebanyak 13 yang selanjutya akan dilakukan one hot encoding"""

#menyimpan variabel kategorik dalam variabel categorical_cols
categorical_cols = data.select_dtypes(include='object')

#menampilkan shape dari data kategorik
categorical_cols.shape

"""Terlihat bahwa jumlah barisnya sebanyak 252.137 dan jumlah kolomnya sebanyak 13"""

#menyimpan variabel numerik dalam variabel categorical_cols
numerical_cols = data.select_dtypes(include=np.number)

#menampilkan shape dari data numerik
numerical_cols.shape

"""Terlihat bahwa jumlah barisnya sebanyak 252.137 dan jumlah kolomnya sebanyak 68"""

#menampilkan unique value pada data kategorik
for col in categorical_cols:
  print(data[col].value_counts())
  print()

"""Dari output di atas, dapat diketahui unique value dari tiap kolom yang bertipe kategork"""

#onehot encoding
onehot = pd.get_dummies(categorical_cols, drop_first=True)

#menampilkan kolom hasil encoding
onehot.head()

#menampilkan kolom encoding
onehot.columns

#menampilkan shape dari hasil encoding
onehot.shape

"""Terlihat bahwa shape datafram encoding jumlah barisnya sebanyak 252.137 dan jumlah kolomnya sebanyak 110"""

#menggabungkan data hasil encoding dengan data numerik
data_final = pd.concat([onehot, numerical_cols],axis=1)

#menampilkan data final
data_final

"""Terlihat bahwa shape data final jumlah barisnya sebanyak 252.137 dan jumlah kolomnya sebanyak 178 kolom.

Pada modelling, data akan dibagi menjadi feature dan target.
"""

#drop kolom TARGET dan menyimpan kolom2 dalam variabel x
x=data_final.drop(['TARGET'], axis=1)

#menyimpan variabel TARGET ke dalam variabel y
y=data_final[('TARGET')]

"""## Random Undersampling"""

#melihat jumlah tiap value pada data y
y.value_counts()

"""Terlihat bahwa data variabel TARGET tidak balance sehingga perlu dilakukan resampling dengan metode undersampling. Metode ini dipilih karena menghasilkan nilai ROC AUC lebih baik dibandingkan dengan oversampling."""

# import random undersampling and other necessary libraries
from collections import Counter
from imblearn.under_sampling import RandomUnderSampler
import warnings
warnings.simplefilter(action='ignore', category=FutureWarning)

# summarize class distribution
print("Before undersampling: ", Counter(y))

# define undersampling strategy
undersample = RandomUnderSampler(sampling_strategy='majority')

# fit and apply the transform
x_under, y_under = undersample.fit_resample(x, y)

# summarize class distribution
print("After undersampling: ", Counter(y_under))

"""## Feature Selection

Feature selection menggunakan metode Mutual Information Gain (MIG) adalah metode untuk memilih fitur yang paling relevan dalam pemodelan statistik. MIG mengukur seberapa banyak informasi yang diberikan oleh suatu fitur tentang kelas target. Dalam feature selection, MIG digunakan untuk memilih fitur yang paling berguna dan relevan untuk memprediksi variabel target. Dengan memilih fitur yang paling informatif, kita dapat meningkatkan akurasi model dan mengurangi dimensi data yang tidak diperlukan, sehingga mempercepat waktu pelatihan dan meminimalkan overfitting.
"""

from sklearn.feature_selection import mutual_info_classif
# menentukan mutual information
mutual_info = mutual_info_classif(x_under, y_under)
mutual_info

mutual_info = pd.Series(mutual_info)
mutual_info.index = x_under.columns
mutual_info.sort_values(ascending=False)

from sklearn.feature_selection import SelectKBest
# select the top 20 important features
sel_twenty_cols = SelectKBest(mutual_info_classif, k=20)
sel_twenty_cols.fit(x_under, y_under)
x_under.columns[sel_twenty_cols.get_support()]

#menyimpan feature importance data train
x_feat=x_under[['NAME_INCOME_TYPE_Working', 'NAME_EDUCATION_TYPE_Higher education',
       'NAME_EDUCATION_TYPE_Secondary / secondary special', 'AMT_CREDIT',
       'AMT_ANNUITY', 'AMT_GOODS_PRICE', 'REGION_POPULATION_RELATIVE',
       'DAYS_BIRTH', 'DAYS_EMPLOYED', 'DAYS_REGISTRATION', 'FLAG_MOBIL',
       'FLAG_EMP_PHONE', 'REGION_RATING_CLIENT', 'EXT_SOURCE_2',
       'EXT_SOURCE_3', 'FLOORSMAX_MODE', 'YEARS_BEGINEXPLUATATION_MEDI',
       'TOTALAREA_MODE', 'DAYS_LAST_PHONE_CHANGE', 'FLAG_DOCUMENT_3']]

"""##Split Data

Selanjutnya, kita split data menjadi data target dan data test untuk training model dan mengetes model yang digunakan.
"""

#import library
from sklearn.model_selection import train_test_split

#split data dengan proporsi data train dan data test sebesar 7:3
x_train, x_test, y_train, y_test = train_test_split(x_feat, y_under, test_size=0.3,random_state=42)

print( x_train.shape,x_test.shape)

"""Diperoleh shape dari data train (30569, 177) dan data test (13101, 177)

##Standardization

Setelah dilakukan undersampling, kemudian dataset akan di standardisasi dengan menggunakan standar scaler. Agar data yang digunakan tidak memiliki penyimpangan yang besar.
"""

#import library
from sklearn.preprocessing import StandardScaler, MinMaxScaler

#standardisasi dengan library StandardScaler()
scaler = StandardScaler()

#standardisasi dengan library StandardScaler()
x_train_scale=scaler.fit_transform(x_train)
x_test_scale=scaler.fit_transform(x_test)

"""##Modelling Undersampling"""

#import library
from sklearn.metrics import classification_report, roc_auc_score

"""###Random Forest"""

#import library
from sklearn.ensemble import RandomForestClassifier
#membuat model dengan metode random forest
rf_under = RandomForestClassifier(random_state=42).fit(x_train_scale, y_train)
#membuat variabel pred
rfpred_under = rf_under.predict(x_test_scale)

#menampilkan nilai ROC AUC
rf_ra = roc_auc_score(y_test, rfpred_under)
print("ROC AUC score for Random Forest: ", rf_ra)

"""### Logistic Regression"""

#import library
from sklearn.linear_model import LogisticRegression
#membuat model dengan metode Logistic Regression
logit_under = LogisticRegression(random_state=42).fit(x_train_scale, y_train)
#membuat variabel pred
logitpred_under = logit_under.predict(x_test_scale)

#menampilkan nilai ROC AUC
logit_ra = roc_auc_score(y_test, rfpred_under)
print("ROC AUC score for Logistic Regression: ",logit_ra )

"""###Gradient Boosting"""

#import library
from sklearn.ensemble import GradientBoostingClassifier
#membuat model dengan metode gradient boosting
gboost_under = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=1, random_state=42).fit(x_train_scale, y_train)
#membuat variabel pred
gbpred_under = gboost_under.predict(x_test_scale)

#menampilkan nilai ROC AUC
gb_ra=roc_auc_score(y_test, gbpred_under)
print("ROC AUC score for Gradient Boosting: ", gb_ra)

"""###Light GBM"""

#import library
from lightgbm import LGBMClassifier
#membuat model dengan metode Light GBM
lgbm_under= LGBMClassifier(random_state=42).fit(x_train_scale, y_train)
#membuat variabel pred
lgbmpred_under = lgbm_under.predict(x_test_scale)

#menampilkan nilai ROC AUC
lgbm_ra=roc_auc_score(y_test, lgbmpred_under)
print("ROC AUC score for Light GBM: ", lgbm_ra)

"""##Evaluation

True Positive (TP)
Merupakan data positif yang diprediksi benar. Contohnya, pasien menderita kanker (class 1) dan dari model yang dibuat memprediksi pasien tersebut menderita kanker (class 1).

True Negative (TN)
Merupakan data negatif yang diprediksi benar. Contohnya, pasien tidak menderita kanker (class 2) dan dari model yang dibuat memprediksi pasien tersebut tidak menderita kanker (class 2).

False Postive (FP) — Type I Error
Merupakan data negatif namun diprediksi sebagai data positif. Contohnya, pasien tidak menderita kanker (class 2) tetapi dari model yang telah memprediksi pasien tersebut menderita kanker (class 1).


False Negative (FN) — Type II Error
Merupakan data positif namun diprediksi sebagai data negatif. Contohnya, pasien menderita kanker (class 1) tetapi dari model yang dibuat memprediksi pasien tersebut tidak menderita kanker (class 2).


Accuracy
Accuracy menggambarkan seberapa akurat model dapat mengklasifikasikan dengan benar. Maka, accuracy merupakan rasio prediksi benar (positif dan negatif) dengan keseluruhan data.

Precision (Positive Predictive Value)
Precision menggambarkan tingkat keakuratan antara data yang diminta dengan hasil prediksi yang diberikan oleh model. Maka, precision merupakan rasio prediksi benar positif dibandingkan dengan keseluruhan hasil yang diprediksi positif.

Recall atau Sensitivity (True Positive Rate)
Recall menggambarkan keberhasilan model dalam menemukan kembali sebuah informasi. Maka, recall merupakan rasio prediksi benar positif dibandingkan dengan keseluruhan data yang benar positif.
"""

#import library
from sklearn.metrics import confusion_matrix
from sklearn.metrics import ConfusionMatrixDisplay
from sklearn.metrics import classification_report
from sklearn.metrics import precision_score
from sklearn.metrics import recall_score
from sklearn.metrics import f1_score
from sklearn.metrics import accuracy_score

"""###Random Forest"""

#menampilkan evaluation metrics
print(classification_report(y_test, rfpred_under))

rf_ac = accuracy_score(y_test, rfpred_under)
rf_pr = precision_score(y_test, rfpred_under)
rf_re = recall_score(y_test, rfpred_under)
rf_f1 = f1_score (y_test, rfpred_under)

#membuat confusion matrix dari model random forest
rf_cm= confusion_matrix(y_test, rfpred_under)

#menampilkan confusion matrix dari model random forest
group_names = ['True Neg','False Pos','False Neg','True Pos']

group_counts = ["{0:0.0f}".format(value) for value in
                rf_cm.flatten()]

group_percentages = ["{0:.2%}".format(value) for value in
                     rf_cm.flatten()/np.sum(rf_cm)]

labels = [f"{v1}\n{v2}\n{v3}" for v1, v2, v3 in
          zip(group_names,group_counts,group_percentages)]

labels = np.asarray(labels).reshape(2,2)

ax = sns.heatmap(rf_cm, annot=labels, fmt='', cmap='Blues')

ax.set_title('Confusion Matrix\nRegression Logistic\n\n');
ax.set_xlabel('\nPredicted Values')
ax.set_ylabel('Actual Values ');

## Ticket labels - List must be in alphabetical order
ax.xaxis.set_ticklabels(['False','True'])
ax.yaxis.set_ticklabels(['False','True'])

## Display the visualization of the Confusion Matrix.
plt.show()

"""###Logistic Regression"""

#menampilkan evaluation metrics
print(classification_report(y_test, logitpred_under))

logit_ac = accuracy_score(y_test, logitpred_under)
logit_pr = precision_score(y_test, logitpred_under)
logit_re = recall_score(y_test, logitpred_under)
logit_f1 = f1_score (y_test, logitpred_under)

#membuat confusion matrix dari model regresi logistik
logit_cm= confusion_matrix(y_test, logitpred_under)

#menampilkan confusion matrix dari model regresi logistik
group_names = ['True Neg','False Pos','False Neg','True Pos']

group_counts = ["{0:0.0f}".format(value) for value in
                logit_cm.flatten()]

group_percentages = ["{0:.2%}".format(value) for value in
                     logit_cm.flatten()/np.sum(logit_cm)]

labels = [f"{v1}\n{v2}\n{v3}" for v1, v2, v3 in
          zip(group_names,group_counts,group_percentages)]

labels = np.asarray(labels).reshape(2,2)

ax = sns.heatmap(logit_cm, annot=labels, fmt='', cmap='Blues')

ax.set_title('Confusion Matrix\nRegression Logistic\n\n');
ax.set_xlabel('\nPredicted Values')
ax.set_ylabel('Actual Values ');

## Ticket labels - List must be in alphabetical order
ax.xaxis.set_ticklabels(['False','True'])
ax.yaxis.set_ticklabels(['False','True'])

## Display the visualization of the Confusion Matrix.
plt.show()

"""###Gradient Boosting"""

#menampilkan evaluation metrics
print(classification_report(y_test, gbpred_under))

#membuat confusion matrix dari model gradient boosting
gb_cm= confusion_matrix(y_test, gbpred_under)

gb_ac = accuracy_score(y_test, gbpred_under)
gb_pr = precision_score(y_test, gbpred_under)
gb_re = recall_score(y_test, gbpred_under)
gb_f1 = f1_score (y_test, gbpred_under)

#menampilkan confusion matrix dari model gradient boosting
group_names = ['True Neg','False Pos','False Neg','True Pos']

group_counts = ["{0:0.0f}".format(value) for value in
                gb_cm.flatten()]

group_percentages = ["{0:.2%}".format(value) for value in
                     gb_cm.flatten()/np.sum(gb_cm)]

labels = [f"{v1}\n{v2}\n{v3}" for v1, v2, v3 in
          zip(group_names,group_counts,group_percentages)]

labels = np.asarray(labels).reshape(2,2)

ax = sns.heatmap(gb_cm, annot=labels, fmt='', cmap='Blues')

ax.set_title('Confusion Matrix\nGradient Boosting\n\n');
ax.set_xlabel('\nPredicted Values')
ax.set_ylabel('Actual Values ');

## Ticket labels - List must be in alphabetical order
ax.xaxis.set_ticklabels(['False','True'])
ax.yaxis.set_ticklabels(['False','True'])

## Display the visualization of the Confusion Matrix.
plt.show()

"""###Light GBM"""

#menampilkan evaluation metrics
print(classification_report(y_test, lgbmpred_under))

#membuat confusion matrix dari model Light GBM
lgbm_cm= confusion_matrix(y_test, lgbmpred_under)

lgbm_ac = accuracy_score(y_test, lgbmpred_under)
lgbm_pr = precision_score(y_test, lgbmpred_under)
lgbm_re = recall_score(y_test, lgbmpred_under)
lgbm_f1 = f1_score (y_test, lgbmpred_under)

#menampilkan confusion matrix dari model Light GBM
group_names = ['True Neg','False Pos','False Neg','True Pos']

group_counts = ["{0:0.0f}".format(value) for value in
                lgbm_cm.flatten()]

group_percentages = ["{0:.2%}".format(value) for value in
                     lgbm_cm.flatten()/np.sum(lgbm_cm)]

labels = [f"{v1}\n{v2}\n{v3}" for v1, v2, v3 in
          zip(group_names,group_counts,group_percentages)]

labels = np.asarray(labels).reshape(2,2)

ax = sns.heatmap(lgbm_cm, annot=labels, fmt='', cmap='Blues')

ax.set_title('Confusion Matrix\nLight GBM\n\n');
ax.set_xlabel('\nPredicted Values')
ax.set_ylabel('Actual Values ');

## Ticket labels - List must be in alphabetical order
ax.xaxis.set_ticklabels(['False','True'])
ax.yaxis.set_ticklabels(['False','True'])

## Display the visualization of the Confusion Matrix.
plt.show()

"""##Comparison Model"""

compa_model = {'Accuracy': [rf_ac, logit_ac, gb_ac, lgbm_ac],
              'ROC AUC': [rf_ra, logit_ra, gb_ra, lgbm_ra],
             'Precision': [rf_pr, logit_pr, gb_pr, lgbm_pr],
             'Recall': [rf_re, logit_re, gb_re, lgbm_re],
             'F1': [rf_f1, logit_f1, gb_f1, lgbm_f1]}

eval_model = pd.DataFrame(compa_model, index=['Random Forest', 'Logistic Regression', 'Gradient Boosting', 'Light GBM'])
eval_model

"""Berdasarkan nilai metriks evaluasi di atas, model terbaik yang dipilih adalah Light GBM.

##Feature Importance
"""

# Mendapatkan feature importance
importance = pd.DataFrame({'feature': x_train.columns, 'importance': lgbm_under.feature_importances_})
importance = importance.sort_values('importance', ascending=False).reset_index(drop=True)
importance

"""Feature yang terpilih sebagai aspek yang mempengaruhi gagal bayar adalah DAYS_EMPLOYED,AMT_ANNUITY,DAYS_BIRTH,AMT_CREDIT,NAME_EDUCATION_TYPE."""